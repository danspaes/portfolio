{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis using NLTK (Natural Language Toolkit) and Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to import all the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import tweepy\n",
    "import random, csv\n",
    "from nltk.tokenize import word_tokenize as wt\n",
    "from unidecode import unidecode\n",
    "import string\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection with twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = None\n",
    "consumer_secret = None\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key=consumer_key, consumer_secret=consumer_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query recent tweets on montreal area within a range of 10 miles and put the results in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "#%23NBC as #NBC\n",
    "#45.5191385,-73.6103499 is montreal geocode\n",
    "for tweet in tweepy.Cursor(api.search, q='@nationalbank', result_type='recent', geocode='45.5191385,-73.6103499,10mi').items(5000):\n",
    "    results.append(tweet)\n",
    "resultDS = pd.DataFrame()\n",
    "\n",
    "resultDS['userName'] = [tweet.user.name for tweet in results]\n",
    "resultDS['tweetText'] = [tweet.text for tweet in results]\n",
    "resultDS['tweetRetweetCt'] = [tweet.retweet_count for tweet in results]\n",
    "resultDS['tweetCreated'] = [tweet.created_at for tweet in results]\n",
    "resultDS['userLocation'] = [tweet.user.location for tweet in results]\n",
    "resultDS['geo'] = [tweet.geo for tweet in results]\n",
    "resultDS['place'] = [tweet.place for tweet in results]\n",
    "resultDS['coordinates'] = [tweet.coordinates for tweet in results]\n",
    "resultDS['userTimezone'] = [tweet.user.time_zone for tweet in results]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start of data preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our alphabet\n",
    "emb_alphabet = 'abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:\\'\"/\\\\|_@#$%^&*~`+-=<>()[]{} '\n",
    "\n",
    "# we associate every character in our alphabet to a number: \n",
    "# e.g. b => 1 d => 3 etc.\n",
    "DICT = {ch: ix for ix, ch in enumerate(emb_alphabet)}\n",
    "\n",
    "# The size of our alphabet (~70)\n",
    "ALPHABET_SIZE = len(emb_alphabet)\n",
    "\n",
    "def reshape_lines(lines):\n",
    "    # Hacky function to make it easier to process the data\n",
    "    data = []\n",
    "    for l in lines:\n",
    "        # \n",
    "        split = l.split('\",\"')\n",
    "        data.append((split[0][1:], split[-1][:-2]))\n",
    "    return data\n",
    "\n",
    "def save_csv(out_file, data):\n",
    "    # Save a file\n",
    "    with open(out_file, 'wb') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(data)\n",
    "    print('Data saved to file: %s' % out_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_datasets(valid_perc=0.05):\n",
    "    \"\"\" Shuffle the datasets \"\"\"\n",
    "    # TRAIN_SET and TEST_SET are respectively the path for \n",
    "    # training.1600000.processed.noemoticon.csv and\n",
    "    # testdata.manual.2009.06.14.csv, this function will create two \n",
    "    # new files called \"valid_set.csv\" and \"train_set.csv\".\n",
    "    \n",
    "    # Make sure the paths exists, otherwise send some help messages...\n",
    "    assert os.path.exists(TRAIN_SET), 'Download the training set at http://help.sentiment140.com/for-students/'\n",
    "    assert os.path.exists(TEST_SET), 'Download the testing set at http://help.sentiment140.com/for-students/'\n",
    "\n",
    "    # Create training and validation set - We take 5% of the training set \n",
    "    # for the validation set by default\n",
    "    print('Creating training & validation set...')\n",
    "\n",
    "    with open(TRAIN_SET, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        random.shuffle(lines)\n",
    "        lines_train = lines[:int(len(lines) * (1 - valid_perc))]\n",
    "        lines_valid = lines[int(len(lines) * (1 - valid_perc)):]\n",
    "\n",
    "    save_csv(PATH + 'datasets/valid_set.csv', reshape_lines(lines_valid))\n",
    "    save_csv(PATH + 'datasets/train_set.csv', reshape_lines(lines_train))\n",
    "\n",
    "    print('Creating testing set...')\n",
    "\n",
    "    with open(TEST_SET, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        random.shuffle(lines)\n",
    "    save_csv(PATH + 'datasets/test_set.csv', reshape_lines(lines))\n",
    "    print('All datasets have been created!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once this is done, we rename the new training and testing set...\n",
    "TRAIN_SET = PATH + 'datasets/train_set.csv'\n",
    "TEST_SET = PATH + 'datasets/test_set.csv'\n",
    "VALID_SET = PATH + 'datasets/valid_set.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def encode_one_hot(self, sentence):\n",
    "        # Convert Sentences to np.array of Shape \n",
    "        # ('sent_length', 'word_length', 'emb_size')\n",
    "\n",
    "        max_word_length = self.max_word_length\n",
    "        sent = []\n",
    "        \n",
    "        # We need to keep track of the maximum length of the sentence in a minibatch\n",
    "        # so that we can pad them with zeros, this is why we return the length of every\n",
    "        # sentences after they are converted to one-hot tensors\n",
    "        SENT_LENGTH = 0\n",
    "        \n",
    "        # Here, we remove any non-printable characters in a sentence (mostly\n",
    "        # non-ASCII characters)\n",
    "        printable = string.printable\n",
    "        encoded_sentence = filter(lambda x: x in printable, sentence)\n",
    "        \n",
    "        # word_tokenize() splits a sentence into an array where each element is\n",
    "        # a word in the sentence, for example, \n",
    "        # \"My name is Charles\" => [\"My\", \"name\", \"is\", Charles\"]\n",
    "        # Unidecode convert characters to utf-8\n",
    "        for word in word_tokenize(unidecode(encoded_sentence)):\n",
    "            \n",
    "            # Encode one word as a matrix of shape [max_word_length x ALPHABET_SIZE]\n",
    "            word_encoding = np.zeros(shape=(max_word_length, ALPHABET_SIZE))\n",
    "            \n",
    "            for i, char in enumerate(word):\n",
    "            \n",
    "                # If the character is not in the alphabet, ignore it    \n",
    "                try:\n",
    "                    char_encoding = DICT[char]\n",
    "                    one_hot = np.zeros(ALPHABET_SIZE)\n",
    "                    one_hot[char_encoding] = 1\n",
    "                    word_encoding[i] = one_hot\n",
    "\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "\n",
    "            sent.append(np.array(word_encoding))\n",
    "            SENT_LENGTH += 1\n",
    "\n",
    "        return np.array(sent), SENT_LENGTH\n",
    "    \n",
    "    def make_minibatch(self, sentences):\n",
    "        # Create a minibatch of sentences and convert sentiment\n",
    "        # to a one-hot vector, also takes care of padding\n",
    "\n",
    "        max_word_length = self.max_word_length\n",
    "        minibatch_x = []\n",
    "        minibatch_y = []\n",
    "        max_length = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            # Append the one-hot encoding of the sentiment to the minibatch of Y\n",
    "            # 0: Negative 1: Positive\n",
    "            minibatch_y.append(np.array([0, 1]) if sentence[:1] == '0' else np.array([1, 0]))\n",
    "\n",
    "            # One-hot encoding of the sentence\n",
    "            one_hot, length = self.encode_one_hot(sentence[2:-1])\n",
    "            \n",
    "            # Calculate maximum_sentence_length\n",
    "            if length >= max_length:\n",
    "                max_length = length\n",
    "            \n",
    "            # Append encoded sentence to the minibatch of X\n",
    "            minibatch_x.append(one_hot)\n",
    "\n",
    "\n",
    "        # data is a np.array of shape ('b', 's', 'w', 'e') we want to\n",
    "        # pad it with np.zeros of shape ('e',) to get \n",
    "        # ('b', 'SENTENCE_MAX_LENGTH', 'WORD_MAX_LENGTH', 'e')\n",
    "        \n",
    "    def numpy_fillna(data):\n",
    "            \"\"\" This is a very useful function that fill the holes in our tensor \"\"\"\n",
    "            \n",
    "            # Get lengths of each row of data\n",
    "            lens = np.array([len(i) for i in data])\n",
    "\n",
    "            # Mask of valid places in each row\n",
    "            mask = np.arange(lens.max()) < lens[:, None]\n",
    "\n",
    "            # Setup output array and put elements from data into masked positions\n",
    "            out = np.zeros(shape=(mask.shape + (max_word_length, ALPHABET_SIZE)),\n",
    "                           dtype='float32')\n",
    "\n",
    "            out[mask] = np.concatenate(data)\n",
    "            return out\n",
    "\n",
    "        # Padding...\n",
    "        minibatch_x = numpy_fillna(minibatch_x)\n",
    "\n",
    "        return minibatch_x, np.array(minibatch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def load_to_ram(self, batch_size):\n",
    "        \"\"\" Load n Rows from File f to Ram \"\"\"\n",
    "        # Returns True if there are still lines in the buffer, \n",
    "        # otherwise returns false - the epoch is over\n",
    "        \n",
    "        self.data = []\n",
    "        n_rows = batch_size\n",
    "        while n_rows > 0:\n",
    "            self.data.append(next(self.file))\n",
    "            n_rows -= 1\n",
    "        if n_rows == 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def iterate_minibatch(self, batch_size, dataset=TRAIN_SET):\n",
    "        \"\"\" Returns Next Batch \"\"\"\n",
    "        \n",
    "        # I realize this could be more \n",
    "        if dataset == TRAIN_SET:\n",
    "            n_samples = 1600000 * 0.95\n",
    "        elif dataset == VALID_SET:\n",
    "            n_samples = 1600000 * 0.05\n",
    "        elif dataset == TEST_SET:\n",
    "            n_samples = 498\n",
    "        \n",
    "        # Number of batches / number of iterations per epoch\n",
    "        n_batch = int(n_samples // batch_size)\n",
    "        \n",
    "        # Creates a minibatch, loads it to RAM and feed it to the network\n",
    "        # until the buffer is empty\n",
    "        for i in range(n_batch):\n",
    "            if self.load_to_ram(batch_size):\n",
    "                inputs, targets = self.make_minibatch(self.data)\n",
    "                yield inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start of operational CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(input_, output_dim, k_h, k_w, name=\"conv2d\"):\n",
    "    \"\"\" Straight-forward convvolutional layer \"\"\"\n",
    "    # w is the kernel, b the bias, no strides and VALID padding\n",
    "\n",
    "    with tf.variable_scope(name):\n",
    "        w = tf.get_variable('w', [k_h, k_w, input_.get_shape()[-1], output_dim])\n",
    "        b = tf.get_variable('b', [output_dim])\n",
    "\n",
    "    return tf.nn.conv2d(input_, w, strides=[1, 1, 1, 1], padding='VALID') + b\n",
    "\n",
    "def tdnn(input_, kernels, kernel_features, scope='TDNN'):\n",
    "    ''' Time Delay Neural Network\n",
    "    :input:           input float tensor of shape \n",
    "                      [(batch_size*num_unroll_steps) x max_word_length x embed_size]\n",
    "    :kernels:         array of kernel sizes\n",
    "    :kernel_features: array of kernel feature sizes (parallel to kernels)\n",
    "    '''\n",
    "    assert len(kernels) == len(kernel_features), 'Kernel and Features must have the same size'\n",
    "\n",
    "    # input_ is a np.array of shape ('b', 'sentence_length', 'max_word_length', 'embed_size') we\n",
    "    # need to convert it to shape ('b * sentence_length', 1, 'max_word_length', 'embed_size') to\n",
    "    # use conv2D\n",
    "    # It might not seem obvious why we need to use this small hack at first sight, the reason\n",
    "    # is that sentence_length will change across the different minibatches, but if we kept it\n",
    "    # as is sentence_length would act as the number of channels in the convnet which NEEDS to\n",
    "    # stay the same\n",
    "    input_ = tf.reshape(input_, [-1, self.max_word_length, ALPHABET_SIZE])\n",
    "    input_ = tf.expand_dims(input_, 1)\n",
    "\n",
    "    layers = []\n",
    "    with tf.variable_scope(scope):\n",
    "        for kernel_size, kernel_feature_size in zip(kernels, kernel_features):\n",
    "            reduced_length = self.max_word_length - kernel_size + 1\n",
    "\n",
    "            # [batch_size * sentence_length x max_word_length x embed_size x kernel_feature_size]\n",
    "            conv = conv2d(input_, kernel_feature_size, 1,\n",
    "                          kernel_size, name=\"kernel_%d\" % kernel_size)\n",
    "\n",
    "            # [batch_size * sentence_length x 1 x 1 x kernel_feature_size]\n",
    "            pool = tf.nn.max_pool(tf.tanh(conv), [1, 1, reduced_length, 1], [1, 1, 1, 1], 'VALID')\n",
    "\n",
    "            layers.append(tf.squeeze(pool, [1, 2]))\n",
    "\n",
    "        if len(kernels) > 1:\n",
    "            output = tf.concat(layers, 1)\n",
    "        else:\n",
    "            output = layers[0]\n",
    "\n",
    "    return output\n",
    "\n",
    "def linear(input_, output_size, scope=None):\n",
    "    \"\"\"\n",
    "    Linear map: output[k] = sum_i(Matrix[k, i] * args[i] ) + Bias[k]\n",
    "    \n",
    "    Args:\n",
    "        args: a tensor or a list of 2D, batch x n, Tensors.\n",
    "        output_size: int, second dimension of W[i].\n",
    "        scope: VariableScope for the created subgraph; defaults to \"Linear\".\n",
    "        \n",
    "    Returns:\n",
    "        A 2D Tensor with shape [batch x output_size] equal to\n",
    "        sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: if some of the arguments has unspecified or wrong shape.\n",
    "    \"\"\"\n",
    "\n",
    "    shape = input_.get_shape().as_list()\n",
    "    if len(shape) != 2:\n",
    "        raise ValueError(\"Linear is expecting 2D arguments: %s\" % str(shape))\n",
    "    if not shape[1]:\n",
    "        raise ValueError(\"Linear expects shape[1] of arguments: %s\" % str(shape))\n",
    "    input_size = shape[1]\n",
    "\n",
    "    # Now the computation.\n",
    "    with tf.variable_scope(scope or \"SimpleLinear\"):\n",
    "        matrix = tf.get_variable(\"Matrix\", [output_size, input_size],\n",
    "                                 dtype=input_.dtype)\n",
    "        bias_term = tf.get_variable(\"Bias\", [output_size], dtype=input_.dtype)\n",
    "\n",
    "    return tf.matmul(input_, tf.transpose(matrix)) + bias_term\n",
    "\n",
    "# Improvements of the model\n",
    "\n",
    "def highway(input_, size, num_layers=1, bias=-2.0, f=tf.nn.relu, scope='Highway'):\n",
    "    \"\"\"Highway Network (cf. http://arxiv.org/abs/1505.00387).\n",
    "    t = sigmoid(Wy + b)\n",
    "    z = t * g(Wy + b) + (1 - t) * y\n",
    "    where g is nonlinearity, t is transform gate, and (1 - t) is carry gate.\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.variable_scope(scope):\n",
    "        for idx in range(num_layers):\n",
    "            g = f(linear(input_, size, scope='highway_lin_%d' % idx))\n",
    "\n",
    "            t = tf.sigmoid(linear(input_, size, scope='highway_gate_%d' % idx) + bias)\n",
    "\n",
    "            output = t * g + (1. - t) * input_\n",
    "            input_ = output\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello=tf.constant('Hello,TensorFlow!')\n",
    "\n",
    "sess=tf.Session()\n",
    "\n",
    "print(sess.run(hello))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
